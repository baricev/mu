MLE-Bench accompanies the paper "MLE-Bench: Evaluating Machine Learning Agents on Machine Learning Engineering" and provides the dataset construction code, evaluation scripts, and baseline agent implementations. The README includes a leaderboard table summarizing agent performance, guidelines for benchmarking with recommended runtime and compute resources, and instructions for running lite or full evaluations. It emphasizes repeating evaluations with multiple seeds to report consistent scores.
